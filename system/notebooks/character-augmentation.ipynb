{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7f4052-74d8-468a-9e7a-3730d527f743",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76cb9dca-7e8d-4237-9aec-4cdb853275a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06eafb5-649f-433f-9f67-3ba845f28517",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f08823e-fb70-4ce8-a7a5-c0f0103c5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_characters_path = r'E:\\GitHub\\smart-parking-system\\dataset\\classification-raw'\n",
    "raw_characters = os.listdir(raw_characters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b574f641-6860-48a8-9616-ea985ad92c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_characters_path = r'E:\\GitHub\\smart-parking-system\\dataset\\classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab1e94d7-7fd4-4b9c-a012-0a58c73d2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_max_size = [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d3139-0a56-425a-8bbe-935b2c2c28df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "356ba1a7-688d-4c3f-bf20-f593e62e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, start = 0, end = 100000):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder)[start:end]:\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f2e6cec-0bc5-4671-b0e1-8cd39866bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_in_folder(output_folder_path, images, start_index = 0):\n",
    "    for idx, img in enumerate(images):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(os.path.join(output_folder_path, str(start_index + idx) + '.png'), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "571bffe0-6966-46b8-b5b0-e56306e640db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images(output_folder_path, augmenter, images, start_index = 0, iterations = 10):\n",
    "    no_samples_batch = len(images)\n",
    "    for iters in range(iterations):\n",
    "        augmented_images=augmenter(images=images)\n",
    "        save_images_in_folder(output_folder_path=output_folder_path, \n",
    "                              images=augmented_images, \n",
    "                              start_index = iters * no_samples_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3632f15-e72c-4efa-a7d4-912f2195cacb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get number of samples for raw characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f04cabf-60bd-42ff-901a-4023777c220c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 directory has 91 samples.\n",
      "1 directory has 73 samples.\n",
      "2 directory has 32 samples.\n",
      "3 directory has 47 samples.\n",
      "4 directory has 18 samples.\n",
      "5 directory has 28 samples.\n",
      "6 directory has 12 samples.\n",
      "7 directory has 15 samples.\n",
      "8 directory has 30 samples.\n",
      "9 directory has 38 samples.\n",
      "A directory has 69 samples.\n",
      "B directory has 168 samples.\n",
      "C directory has 44 samples.\n",
      "D directory has 20 samples.\n",
      "E directory has 6 samples.\n",
      "F directory has 20 samples.\n",
      "G directory has 16 samples.\n",
      "H directory has 13 samples.\n",
      "I directory has 28 samples.\n",
      "J directory has 1 samples.\n",
      "K directory has 17 samples.\n",
      "L directory has 44 samples.\n",
      "M directory has 16 samples.\n",
      "N directory has 13 samples.\n",
      "O directory has 14 samples.\n",
      "P directory has 28 samples.\n",
      "R directory has 31 samples.\n",
      "S directory has 44 samples.\n",
      "T directory has 38 samples.\n",
      "U directory has 2 samples.\n",
      "V directory has 40 samples.\n",
      "W directory has 17 samples.\n",
      "X directory has 17 samples.\n",
      "Y directory has 19 samples.\n",
      "Z directory has 5 samples.\n"
     ]
    }
   ],
   "source": [
    "raw_no_samples = {}\n",
    "for raw_character in raw_characters:\n",
    "    print(f'{raw_character} directory has {len(os.listdir(os.path.join(raw_characters_path, raw_character)))} samples.')\n",
    "    raw_no_samples[str(raw_character)] = len(os.listdir(os.path.join(raw_characters_path, raw_character)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7d66455-f667-46fc-95b4-45ec07a8a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 91, '1': 73, '2': 32, '3': 47, '4': 18, '5': 28, '6': 12, '7': 15, '8': 30, '9': 38, 'A': 69, 'B': 168, 'C': 44, 'D': 20, 'E': 6, 'F': 20, 'G': 16, 'H': 13, 'I': 28, 'J': 1, 'K': 17, 'L': 44, 'M': 16, 'N': 13, 'O': 14, 'P': 28, 'R': 31, 'S': 44, 'T': 38, 'U': 2, 'V': 40, 'W': 17, 'X': 17, 'Y': 19, 'Z': 5}\n"
     ]
    }
   ],
   "source": [
    "print(raw_no_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f035c9-2098-4e1d-ad5f-28b56a92c36c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add padding to images to make them the same size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3013a-a05a-49a0-9495-23aedfc3ca04",
   "metadata": {},
   "source": [
    "### Search for the maximum widht and height that we'll later use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03818548-fb2d-4f8a-ae56-d8a3398dc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_character in raw_characters:\n",
    "    for raw_sample in os.listdir(os.path.join(raw_characters_path, raw_character)):\n",
    "        image = Image.open(os.path.join(raw_characters_path, raw_character, raw_sample))\n",
    "        width, height = image.size\n",
    "        if width > raw_max_size[0]:\n",
    "            raw_max_size[0] = width\n",
    "        if height > raw_max_size[1]:\n",
    "            raw_max_size[1] = height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "280e1b5c-65bb-42ac-8027-7354da5f2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_max_size = [195, 256]\n"
     ]
    }
   ],
   "source": [
    "print(f'{raw_max_size = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f631ec91-e232-40e4-9b37-d93ef736b096",
   "metadata": {},
   "source": [
    "### Add padding to all images to match the maximum size of a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8d69230-ee07-4c3d-bf74-ffe1f410f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_character in raw_characters:\n",
    "    for raw_sample in os.listdir(os.path.join(raw_characters_path, raw_character)):\n",
    "        image = Image.open(os.path.join(raw_characters_path, raw_character, raw_sample))\n",
    "        width, height = image.size\n",
    "\n",
    "        x_axis_offset = (raw_max_size[0] - width) / 2\n",
    "        y_axis_offset = (raw_max_size[1] - height) / 2\n",
    "        new_width = width + int((2 * x_axis_offset))\n",
    "        new_height = height + int((2 * y_axis_offset))\n",
    "\n",
    "        result = Image.new(image.mode, (new_width, new_height), (255, 255, 255))\n",
    "        result.paste(image, (int(x_axis_offset), int(y_axis_offset)))\n",
    "        result.save(os.path.join(raw_characters_path, raw_character, raw_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed542c6c-b692-44d0-9e09-1cccb61a0091",
   "metadata": {},
   "source": [
    "## Add data augmentation to the raw padded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb37b0-14a4-4cfb-a84a-cb4fc13a2e66",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58a6eba1-319d-428e-956b-9e7ebc60c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n",
    "# image.\n",
    "sometimes = lambda aug: iaa.Sometimes(0.3, aug)\n",
    "\n",
    "# Define our sequence of augmentation steps that will be applied to every image.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        # crop and pad some of the images by 0-10% of their height/width\n",
    "        sometimes(iaa.CropAndPad(percent=(0, 0.1),\n",
    "                                 pad_mode=[\"constant\", \"edge\"],\n",
    "                                 pad_cval=(254, 255)\n",
    "        )),\n",
    "\n",
    "        # Apply affine transformations to some of the images\n",
    "        # - scale to 80-120% of image height/width (each axis independently)\n",
    "        # - translate by -20 to +20 relative to height/width (per axis)\n",
    "        # - rotate by -45 to +45 degrees\n",
    "        # - shear by -16 to +16 degrees\n",
    "        # - order: use nearest neighbour or bilinear interpolation (fast)\n",
    "        # - mode: use any available mode to fill newly created pixels\n",
    "        #         see API or scikit-image for which modes are available\n",
    "        # - cval: if the mode is constant, then use a random brightness\n",
    "        #         for the newly created pixels (e.g. sometimes black,\n",
    "        #         sometimes white)\n",
    "        sometimes(iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.1), \"y\": (0.8, 1.1)},\n",
    "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "            rotate=(-3, 3),\n",
    "            shear=(-5, 5),\n",
    "            order=[0, 1],\n",
    "            cval=(254, 255),\n",
    "            mode=ia.ALL\n",
    "        )),\n",
    "\n",
    "        #\n",
    "        # Execute 0 to 3 of the following (less important) augmenters per\n",
    "        # image. Don't execute all of them, as that would often be way too\n",
    "        # strong.\n",
    "        #\n",
    "        iaa.SomeOf((0, 3),\n",
    "            [\n",
    "                iaa.Dropout(p=(0, 0.1)),\n",
    "\n",
    "                # Blur each image with varying strength using\n",
    "                # gaussian blur (sigma between 0 and 3.0),\n",
    "                # average/uniform blur (kernel size between 2x2 and 7x7)\n",
    "                # median blur (kernel size between 3x3 and 11x11).\n",
    "                iaa.OneOf([\n",
    "                    iaa.GaussianBlur((0, .3)),\n",
    "                    iaa.AverageBlur(k=(2, 3))\n",
    "                ]),\n",
    "\n",
    "                # Sharpen each image, overlay the result with the original\n",
    "                # image using an alpha between 0 (no sharpening) and 1\n",
    "                # (full sharpening effect).\n",
    "                iaa.Sharpen(alpha=(0.2, 0.8), lightness=(0.75, 1.25)),\n",
    "\n",
    "                # Search in some images either for all edges or for\n",
    "                # directed edges. These edges are then marked in a black\n",
    "                # and white image and overlayed with the original image\n",
    "                # using an alpha of 0 to 0.7.\n",
    "                sometimes(iaa.OneOf([\n",
    "                    iaa.EdgeDetect(alpha=(0, 0.1)),\n",
    "                    iaa.DirectedEdgeDetect(\n",
    "                        alpha=(0, 0.1), direction=(0.0, 1.0)\n",
    "                    ),\n",
    "                ])),\n",
    "\n",
    "                # Add a value of -10 to 10 to each pixel.\n",
    "                iaa.Add((-10, 10)),\n",
    "\n",
    "                # Change brightness of images (50-150% of original value).\n",
    "                iaa.Multiply((0.5, 1.2)),\n",
    "\n",
    "                # Improve or worsen the contrast of images.\n",
    "                iaa.LinearContrast((0.5, 1.6)),\n",
    "\n",
    "                # Convert each image to grayscale and then overlay the\n",
    "                # result with the original with random alpha. I.e. remove\n",
    "                # colors with varying strengths.\n",
    "                iaa.Grayscale(alpha=(0.0, 1.0)),\n",
    "\n",
    "                # In some images distort local areas with varying strength.\n",
    "                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.03)))\n",
    "            ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "    ],\n",
    "    # do all of the above augmentations in random order\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb2538-50e5-4020-8022-54870fe9f5a7",
   "metadata": {},
   "source": [
    "### Augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd0dc82d-f939-4de7-a052-3de6fecbee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2200 samples per clasa are trebui sa fie ok\n",
    "aug_iters = {}\n",
    "for character in list(raw_no_samples.keys()):\n",
    "    aug_iters[character] = round(2200 / raw_no_samples[character])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4118ef1-2e47-4846-828d-a5d5055e10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for character in list(aug_iters.keys()):\n",
    "    imgs = load_images_from_folder(os.path.join(raw_characters_path, character))\n",
    "    augment_images(output_folder_path=os.path.join(augmented_characters_path, character), \n",
    "                   augmenter=seq, \n",
    "                   images=imgs, \n",
    "                   iterations=aug_iters[character])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddeee1e-1641-4cc3-9ff1-0a64f2450440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
